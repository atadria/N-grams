{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05607cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "import random\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91c4bec",
   "metadata": {},
   "source": [
    "# Scrap data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f1d6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_URL = 'https://wolnelektury.pl/media/book/txt/nasza-szkapa.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ec8f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = requests.get(DATA_URL).text\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6741e985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706a2cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'Maria Konopnicka Nasza szkapa ISBN 978-83-288-2363-1'\n",
    "\n",
    "text = text.replace('\\r\\n', ' ')\n",
    "\n",
    "text = text.replace('  ', ' ')\n",
    "text = text[len(prefix):]\n",
    "text = text.split('-----')[0]\n",
    "\n",
    "# in python 3.9 + \n",
    "# text.removeprefix(prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7746762",
   "metadata": {},
   "source": [
    "## Split data into sentences \n",
    "Test 2 methods:\n",
    "\n",
    "* using re module\n",
    "\n",
    "* char by char iteration over text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23164962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_split_using_re(text):\n",
    "    sentence_terminators = '.!?'\n",
    "    sentence_terminators = re.compile('[.!?]')\n",
    "    return sentence_terminators.split(text)\n",
    "\n",
    "\n",
    "def sentence_split(text):\n",
    "    sentence_terminators = '.!?'\n",
    "    current_sentence = ''\n",
    "    sentences = []\n",
    "    for char in text:\n",
    "        if char in sentence_terminators:\n",
    "            if len(current_sentence) > 0:\n",
    "                sentences.append(current_sentence)\n",
    "                current_sentence = ''\n",
    "        else:\n",
    "            current_sentence += char\n",
    "    if len(current_sentence) > 0:\n",
    "        sentences.append(current_sentence)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86003838",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "sentence_split(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d10211f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "sentence_split_using_re(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21afc55e",
   "metadata": {},
   "source": [
    "## Tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf615af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    for punct in string.punctuation:\n",
    "        sentence = sentence.replace(punct, ' ')\n",
    "    tokenized = [t for t in sentence.lower().split() if t.isalpha() and len(t)]\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d7f18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = [tokenize(sentence) for sentence in sentence_split(text)]\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019ccb17",
   "metadata": {},
   "source": [
    "# Create n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d69f697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(tokens, n):\n",
    "    t = ['<START>'] * (n - 1) + tokens\n",
    "    return [(tuple(t[i:i+n-1]), t[i+n]) for i in range(len(t)-n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639396a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grams = [get_ngrams(sentence, 3) for sentence in tokenized]\n",
    "# Counter(n_grams).most_common()\n",
    "n_grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476f2c24",
   "metadata": {},
   "source": [
    "# N-gram model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9cea3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramModel(object):\n",
    "\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        self.context = defaultdict(list)\n",
    "        self.ngram_counter = Counter()\n",
    "\n",
    "    def update(self, sentence: str) -> None:\n",
    "        ngrams = get_ngrams(tokenize(sentence), self.n)\n",
    "        for ngram in ngrams:\n",
    "            self.ngram_counter[ngram] += 1\n",
    "            self.context[ngram[0]].append(ngram[1])\n",
    "                \n",
    "    def prob(self, context, token):\n",
    "        \"\"\"\n",
    "        Calculates probability of a candidate token to be generated given a context\n",
    "        :return: conditional probability\n",
    "        \"\"\"\n",
    "        count_of_token = self.ngram_counter[(context, token)]\n",
    "        count_of_context = len(self.context[context])\n",
    "        if count_of_context > 0: \n",
    "            return count_of_token / count_of_context\n",
    "        return 0.0\n",
    "    \n",
    "    def random_token(self, context):\n",
    "        \"\"\"\n",
    "        Given a context we \"semi-randomly\" select the next word to append in a sequence\n",
    "        :param context:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        r = random.random()\n",
    "        map_to_probs = {}\n",
    "        token_of_interest = self.context[context]\n",
    "        for token in token_of_interest:\n",
    "            map_to_probs[token] = self.prob(context, token)\n",
    "\n",
    "        summ = 0\n",
    "        for token in sorted(map_to_probs):\n",
    "            summ += map_to_probs[token]\n",
    "            if summ > r:\n",
    "                return token\n",
    "\n",
    "    def generate_text(self, token_count: int):\n",
    "        \"\"\"\n",
    "        :param token_count: number of words to be produced\n",
    "        :return: generated text\n",
    "        \"\"\"\n",
    "        n = self.n\n",
    "        context_queue = (n - 1) * ['<START>']\n",
    "        result = []\n",
    "        while len(result) < token_count:\n",
    "            predicted_token = self.random_token(tuple(context_queue))\n",
    "            if predicted_token:\n",
    "                result.append(predicted_token)\n",
    "            else:\n",
    "                predicted_token = '<START>'\n",
    "            context_queue.pop(0)\n",
    "            context_queue.append(predicted_token)\n",
    "        return ' '.join(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7874e454",
   "metadata": {},
   "source": [
    "## Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb74ad7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = NgramModel(2)\n",
    "for sentence in sentence_split(text):\n",
    "    model.update(sentence) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f197e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.ngram_counter.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7829b5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate_text(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
